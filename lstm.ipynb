{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "4l6L01MPs7hh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "W-Sk3z2JsyAQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter # to calculate word frequencies\n",
        "from itertools import product # Creating a combination for grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loading and preprocessing"
      ],
      "metadata": {
        "id": "ymZ2C4p-tGwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# product reviews\n",
        "text = \"\"\" Bu ürün beklentimi fazlasıyla karşıladı!\n",
        "Malzeme kalitesi, işçilik... hepsi beklediğimden iyi.\n",
        "Kutu içeriği eksiksizdi; kurulumu da oldukça kolaydı.\n",
        "Performansı günlük kullanımda çok akıcı, sessiz ve stabil.\n",
        "Kısacası: bu fiyata kesinlikle çok iyi tekrar tercih ederim! \"\"\"\n",
        "\n",
        "# data preprocessing:\n",
        "# get rid of punctuation.\n",
        "# convert to lowercase.\n",
        "# split words\n",
        "\n",
        "words = text.replace('.', '').replace(',', '').replace('!', '').replace(':', '').replace(';', '').lower().split()\n",
        "\n",
        "# calculate word frequencies and create indexes\n",
        "\n",
        "word_count = Counter(words)\n",
        "vocab = sorted(word_count, key=word_count.get, reverse=True) # sort word frequency from smallest to largest\n",
        "word_2_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_2_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "# training data preparation\n",
        "\n",
        "data = [(words[i],words[i+1]) for i in range(len(words) - 1)]\n"
      ],
      "metadata": {
        "id": "MW_b8Zenwf5b"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lstm model definition"
      ],
      "metadata": {
        "id": "IQhB1Km6tI5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "\n",
        "      super(LSTM, self).__init__() # calling a superclass's constructor\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n",
        "      self.lstm = nn.LSTM(embedding_dim, hidden_dim) # lstm layer\n",
        "      self.fc = nn.Linear(hidden_dim, vocab_size) # fully connected layer\n",
        "\n",
        "  def forward(self, x): # feed forward function\n",
        "\n",
        "      x = self.embedding(x) # embedding layer\n",
        "      lstm_out, _ = self.lstm(x.view(1,1,-1)) # lstm layer\n",
        "      output = self.fc(lstm_out.view(1,-1)) # fully connected layer\n",
        "      return output\n",
        "\n",
        "model = LSTM(len(vocab), embedding_dim = 8, hidden_dim = 32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7J7RMwRg5XxD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hyperparameter tuning"
      ],
      "metadata": {
        "id": "UTc6iRBVtisR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word list -> tensor\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    return torch.tensor([to_ix[w] for w in seq], dtype=torch.long)\n",
        "\n",
        "# Let's determine the hyperparameter tuning combinations\n",
        "embedding_sizes = [8, 16] # Embedding sizes to try\n",
        "hidden_sizes = [32, 64] # hidden layer sizes to be tested\n",
        "learning_rates = [0.01, 0.005] # learning rate\n",
        "\n",
        "best_loss = float('inf') # to keep the loss value to a minimum\n",
        "best_params = {} # to keep the parameters that give the lowest loss value\n",
        "\n",
        "print(\"Hyperparameter Tuning\")\n",
        "\n",
        "# grid search\n",
        "for emb_size, hidden_size, lr in product(embedding_sizes, hidden_sizes, learning_rates):\n",
        "    print(f\"Attempt: Embedding Size: {emb_size}, Hidden Size: {hidden_size}, Learning Rate: {lr}\")\n",
        "\n",
        "    # modeli tanımla\n",
        "    model = LSTM(len(vocab), emb_size, hidden_size) # create model with selected parameters\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    epochs = 100\n",
        "    total_loss = 0\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0 # reset loss at epoch start\n",
        "        for word, next_word in data:\n",
        "            model.zero_grad() # reset gradients\n",
        "            input_tensor = prepare_sequence([word], word_2_index) # create input tensor\n",
        "            target_tensor = prepare_sequence([next_word], word_2_index) # create output tensor\n",
        "            output = model(input_tensor) # model çıktısını al\n",
        "            loss = loss_function(output, target_tensor)\n",
        "            loss.backward() # apply backpropagation\n",
        "            optimizer.step() # update parameters\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch}, Loss: {epoch_loss:.5f}\")\n",
        "        total_loss = epoch_loss\n",
        "\n",
        "    # save best model\n",
        "    if total_loss < best_loss:\n",
        "        best_loss = total_loss\n",
        "        best_params = {'embedding_dim': emb_size, 'hidden_dim': hidden_size, 'learning_rate': lr}\n",
        "    print()\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMrCnSEb7bOl",
        "outputId": "d43c7b5e-ef31-4733-90db-eb0478583297"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter Tuning\n",
            "Attempt: Embedding Size: 8, Hidden Size: 32, Learning Rate: 0.01\n",
            "Epoch: 0, Loss: 121.90928\n",
            "Epoch: 10, Loss: 8.70070\n",
            "Epoch: 20, Loss: 6.51993\n",
            "Epoch: 30, Loss: 6.00152\n",
            "Epoch: 40, Loss: 5.73674\n",
            "Epoch: 50, Loss: 5.60921\n",
            "Epoch: 60, Loss: 5.46220\n",
            "Epoch: 70, Loss: 5.37964\n",
            "Epoch: 80, Loss: 5.31057\n",
            "Epoch: 90, Loss: 5.24786\n",
            "\n",
            "Attempt: Embedding Size: 8, Hidden Size: 32, Learning Rate: 0.005\n",
            "Epoch: 0, Loss: 119.42760\n",
            "Epoch: 10, Loss: 17.19873\n",
            "Epoch: 20, Loss: 7.16656\n",
            "Epoch: 30, Loss: 6.00794\n",
            "Epoch: 40, Loss: 5.59127\n",
            "Epoch: 50, Loss: 5.37616\n",
            "Epoch: 60, Loss: 5.24624\n",
            "Epoch: 70, Loss: 5.16843\n",
            "Epoch: 80, Loss: 5.10373\n",
            "Epoch: 90, Loss: 5.05018\n",
            "\n",
            "Attempt: Embedding Size: 8, Hidden Size: 64, Learning Rate: 0.01\n",
            "Epoch: 0, Loss: 121.12013\n",
            "Epoch: 10, Loss: 7.73745\n",
            "Epoch: 20, Loss: 6.48762\n",
            "Epoch: 30, Loss: 5.96138\n",
            "Epoch: 40, Loss: 5.79164\n",
            "Epoch: 50, Loss: 5.56153\n",
            "Epoch: 60, Loss: 5.43906\n",
            "Epoch: 70, Loss: 5.35139\n",
            "Epoch: 80, Loss: 5.20299\n",
            "Epoch: 90, Loss: 5.20615\n",
            "\n",
            "Attempt: Embedding Size: 8, Hidden Size: 64, Learning Rate: 0.005\n",
            "Epoch: 0, Loss: 119.54893\n",
            "Epoch: 10, Loss: 10.08405\n",
            "Epoch: 20, Loss: 6.61570\n",
            "Epoch: 30, Loss: 5.99230\n",
            "Epoch: 40, Loss: 5.70864\n",
            "Epoch: 50, Loss: 5.53264\n",
            "Epoch: 60, Loss: 5.42463\n",
            "Epoch: 70, Loss: 5.32747\n",
            "Epoch: 80, Loss: 5.24621\n",
            "Epoch: 90, Loss: 5.19151\n",
            "\n",
            "Attempt: Embedding Size: 16, Hidden Size: 32, Learning Rate: 0.01\n",
            "Epoch: 0, Loss: 120.91864\n",
            "Epoch: 10, Loss: 7.57376\n",
            "Epoch: 20, Loss: 6.20604\n",
            "Epoch: 30, Loss: 5.83873\n",
            "Epoch: 40, Loss: 5.65163\n",
            "Epoch: 50, Loss: 5.52773\n",
            "Epoch: 60, Loss: 5.38655\n",
            "Epoch: 70, Loss: 5.25368\n",
            "Epoch: 80, Loss: 5.21023\n",
            "Epoch: 90, Loss: 5.17325\n",
            "\n",
            "Attempt: Embedding Size: 16, Hidden Size: 32, Learning Rate: 0.005\n",
            "Epoch: 0, Loss: 119.38196\n",
            "Epoch: 10, Loss: 12.12700\n",
            "Epoch: 20, Loss: 6.65971\n",
            "Epoch: 30, Loss: 5.85013\n",
            "Epoch: 40, Loss: 5.53126\n",
            "Epoch: 50, Loss: 5.35539\n",
            "Epoch: 60, Loss: 5.26274\n",
            "Epoch: 70, Loss: 5.17925\n",
            "Epoch: 80, Loss: 5.12851\n",
            "Epoch: 90, Loss: 5.08213\n",
            "\n",
            "Attempt: Embedding Size: 16, Hidden Size: 64, Learning Rate: 0.01\n",
            "Epoch: 0, Loss: 121.13577\n",
            "Epoch: 10, Loss: 7.21683\n",
            "Epoch: 20, Loss: 6.10327\n",
            "Epoch: 30, Loss: 5.74024\n",
            "Epoch: 40, Loss: 5.52295\n",
            "Epoch: 50, Loss: 5.37855\n",
            "Epoch: 60, Loss: 5.25776\n",
            "Epoch: 70, Loss: 5.19423\n",
            "Epoch: 80, Loss: 5.10883\n",
            "Epoch: 90, Loss: 5.01114\n",
            "\n",
            "Attempt: Embedding Size: 16, Hidden Size: 64, Learning Rate: 0.005\n",
            "Epoch: 0, Loss: 119.79107\n",
            "Epoch: 10, Loss: 7.95725\n",
            "Epoch: 20, Loss: 6.22852\n",
            "Epoch: 30, Loss: 5.80356\n",
            "Epoch: 40, Loss: 5.56054\n",
            "Epoch: 50, Loss: 5.43020\n",
            "Epoch: 60, Loss: 5.33162\n",
            "Epoch: 70, Loss: 5.26676\n",
            "Epoch: 80, Loss: 5.19128\n",
            "Epoch: 90, Loss: 5.10334\n",
            "\n",
            "Best Parameters: {'embedding_dim': 8, 'hidden_dim': 32, 'learning_rate': 0.005}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lstm training"
      ],
      "metadata": {
        "id": "ysNB2G1Ytmvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = LSTM(len(vocab), best_params['embedding_dim'], best_params['hidden_dim'])\n",
        "optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
        "loss_function = nn.CrossEntropyLoss() # entropy loss function\n",
        "\n",
        "print(\"Final model training\")\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for word, next_word in data:\n",
        "        final_model.zero_grad()\n",
        "        input_tensor = prepare_sequence([word], word_2_index)\n",
        "        target_tensor = prepare_sequence([next_word], word_2_index)\n",
        "        output = final_model(input_tensor)\n",
        "        loss = loss_function(output, target_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Final Model Epoch: {epoch}, Loss: {epoch_loss:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEiM2OqDBP3L",
        "outputId": "caada66e-bf56-4dbe-cc50-76281689f4ea"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model training\n",
            "Final Model Epoch: 0, Loss: 119.23145\n",
            "Final Model Epoch: 10, Loss: 16.84916\n",
            "Final Model Epoch: 20, Loss: 7.16482\n",
            "Final Model Epoch: 30, Loss: 6.00450\n",
            "Final Model Epoch: 40, Loss: 5.58995\n",
            "Final Model Epoch: 50, Loss: 5.38060\n",
            "Final Model Epoch: 60, Loss: 5.25336\n",
            "Final Model Epoch: 70, Loss: 5.16809\n",
            "Final Model Epoch: 80, Loss: 5.10590\n",
            "Final Model Epoch: 90, Loss: 5.07441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing and evaluation"
      ],
      "metadata": {
        "id": "6RgMIdDzvOC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word prediction function: initial development and generate n words\n",
        "def predict_sequence(start_word, num_words):\n",
        "    current_word = start_word # the current word is set as the starting word\n",
        "    output_sequence = [current_word] # output sequence\n",
        "\n",
        "    for _ in range(num_words): # guess the specified number of words\n",
        "        with torch.no_grad(): # without performing gradient calculation\n",
        "            input_tensor = prepare_sequence([current_word], word_2_index) # tensor transform from word\n",
        "            output = final_model(input_tensor) # call model\n",
        "            predicted_index = torch.argmax(output).item() # get the index of the word with the highest probability\n",
        "            predicted_word = index_2_word[predicted_index] # returns the word corresponding to the index\n",
        "            output_sequence.append(predicted_word)\n",
        "            current_word = predicted_word # update current words for next prediction\n",
        "\n",
        "    return output_sequence # return the predicted word string\n",
        "\n",
        "\"\"\"\n",
        "Bu ürün beklentimi fazlasıyla karşıladı!\n",
        "Malzeme kalitesi, işçilik... hepsi beklediğimden iyi.\n",
        "Kutu içeriği eksiksizdi; kurulumu da oldukça kolaydı.\n",
        "Performansı günlük kullanımda çok akıcı, sessiz ve stabil.\n",
        "Kısacası: bu fiyata kesinlikle çok iyi tekrar tercih ederim!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "start_word = \"ürün\"\n",
        "num_predictions = 3\n",
        "predict_sequence = predict_sequence(start_word, num_predictions)\n",
        "print(\" \".join(predict_sequence))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urA8b-QGCkNJ",
        "outputId": "ac92fe74-d1d8-45d6-caf9-a80cdc4f3578"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ürün beklentimi fazlasıyla karşıladı\n"
          ]
        }
      ]
    }
  ]
}